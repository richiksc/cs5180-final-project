{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four Rooms Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENSE_REWARDS = True\n",
    "\n",
    "class FourRooms(object):\n",
    "    def __init__(self):\n",
    "        # The grid for the Four Rooms domain\n",
    "        self.grid = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "        # Observation (state) space consists of all empty cells\n",
    "        # To improve interpretability, we flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        self.observation_space = np.argwhere(self.grid == 0.0).tolist()  # Fine all empty cells\n",
    "        self.observation_space = self.arr_coords_to_four_room_coords(self.observation_space)\n",
    "\n",
    "        # Action space\n",
    "        self.action_movement = {0: np.array([0, 1]),  # up\n",
    "                                1: np.array([0, -1]),  # down\n",
    "                                2: np.array([-1, 0]),  # left\n",
    "                                3: np.array([1, 0])}  # right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Start location\n",
    "        self.start_location = [0, 0]\n",
    "\n",
    "        # Goal location\n",
    "        self.goal_location = [10, 10]\n",
    "\n",
    "        # Wall locations\n",
    "        self.walls = np.argwhere(self.grid == 1.0).tolist()  # find all wall cells\n",
    "        self.walls = self.arr_coords_to_four_room_coords(self.walls)  # convert to Four Rooms coordinates\n",
    "\n",
    "        # This is an episodic task, with a timeout of 459 steps\n",
    "        self.max_time_steps = 459\n",
    "\n",
    "        # Tracking variables during a single episode\n",
    "        self.agent_location = None  # Track the agent's location in one episode.\n",
    "        self.action = None  # Track the agent's action\n",
    "        self.t = 0  # Track the current time step in one episode\n",
    "\n",
    "    @staticmethod\n",
    "    def arr_coords_to_four_room_coords(arr_coords_list):\n",
    "        \"\"\"\n",
    "        Function converts the array coordinates ((row, col), origin is top left)\n",
    "        to the Four Rooms coordinates ((x, y), origin is bottom left)\n",
    "        E.g., The coordinates (0, 0) in the numpy array is mapped to (0, 10) in the Four Rooms coordinates.\n",
    "        Args:\n",
    "            arr_coords_list (list): List variable consisting of tuples of locations in the numpy array\n",
    "\n",
    "        Return:\n",
    "            four_room_coords_list (list): List variable consisting of tuples of converted locations in the\n",
    "                                          Four Rooms environment.\n",
    "        \"\"\"\n",
    "        # Flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        four_room_coords_list = [(column_idx, 10 - row_idx) for (row_idx, column_idx) in arr_coords_list]\n",
    "        return four_room_coords_list\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the agent's location to the start location\n",
    "        self.agent_location = self.start_location\n",
    "\n",
    "        # Reset the timeout tracker to be 0\n",
    "        self.t = 0\n",
    "\n",
    "        # Reset the information\n",
    "        info = {}\n",
    "        return self.agent_location, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (int): Int variable (i.e., 0 for \"up\"). See self.action_movement above for more details.\n",
    "        \"\"\"\n",
    "        # With probability 0.8, the agent takes the correct direction.\n",
    "        # With probability 0.2, the agent takes one of the two perpendicular actions.\n",
    "        # For example, if the correct action is \"LEFT\", then\n",
    "        #     - With probability 0.8, the agent takes action \"LEFT\";\n",
    "        #     - With probability 0.1, the agent takes action \"UP\";\n",
    "        #     - With probability 0.1, the agent takes action \"DOWN\".\n",
    "        if np.random.uniform() < 0.2:\n",
    "            if action == 2 or action == 3:\n",
    "                action = np.random.choice([0, 1], 1)[0]\n",
    "            else:\n",
    "                action = np.random.choice([2, 3], 1)[0]\n",
    "\n",
    "        # Convert the agent's location to array\n",
    "        loc_arr = np.array(self.agent_location)\n",
    "\n",
    "        # Convert the action name to movement array\n",
    "        act_arr = self.action_movement[action]\n",
    "\n",
    "        # Compute the agent's next location\n",
    "        next_agent_location = np.clip(loc_arr + act_arr,\n",
    "                                      a_min=np.array([0, 0]),\n",
    "                                      a_max=np.array([10, 10])).tolist()\n",
    "\n",
    "        # Check if the agent crashes into walls; if so, it stays at the current location.\n",
    "        if tuple(next_agent_location) in self.walls:\n",
    "            next_agent_location = self.agent_location\n",
    "\n",
    "        # Compute the reward (1 iff next state is goal location)\n",
    "        if DENSE_REWARDS:\n",
    "            reward = 0.0 if next_agent_location == self.goal_location else -1.0\n",
    "        else:\n",
    "            reward = 1.0 if next_agent_location == self.goal_location else 0.0\n",
    "\n",
    "        # Check termination/truncation\n",
    "        # If agent reaches the goal, reward = 1, terminated = True\n",
    "        # If timeout is reached, reward = 0, truncated = True\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        if reward == 1.0:\n",
    "            terminated = True\n",
    "        elif self.t == self.max_time_steps:\n",
    "            truncated = True\n",
    "\n",
    "        # Update the agent's location, action, and time step trackers\n",
    "        self.agent_location = next_agent_location\n",
    "        self.action = action\n",
    "        self.t += 1\n",
    "\n",
    "        return next_agent_location, reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        # Plot the agent and the goal\n",
    "        # empty cell = 0\n",
    "        # wall cell = 1\n",
    "        # agent cell = 2\n",
    "        # goal cell = 3\n",
    "        plot_arr = self.grid.copy()\n",
    "        plot_arr[10 - self.agent_location[1], self.agent_location[0]] = 2\n",
    "        plot_arr[10 - self.goal_location[1], self.goal_location[0]] = 3\n",
    "        plt.clf()\n",
    "        plt.title(f\"state={self.agent_location}, act={self.action_movement[self.action]}\")\n",
    "        plt.imshow(plot_arr)\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        env = FourRooms()\n",
    "        state, info = env.reset()\n",
    "\n",
    "        for _ in range(1000):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            env.render()\n",
    "            if terminated or truncated:\n",
    "                state, info = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "# Un-comment to run test function\n",
    "# FourRooms.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, *, window_size = 50):\n",
    "    \"\"\"Smooths 1-D data array using a moving average.\n",
    "\n",
    "    Args:\n",
    "        data: 1-D numpy.array\n",
    "        window_size: Size of the smoothing window\n",
    "\n",
    "    Returns:\n",
    "        smooth_data: A 1-d numpy.array with the same size as data\n",
    "    \"\"\"\n",
    "    assert data.ndim == 1\n",
    "    kernel = np.ones(window_size)\n",
    "    smooth_data = np.convolve(data, kernel) / np.convolve(\n",
    "        np.ones_like(data), kernel\n",
    "    )\n",
    "    return smooth_data[: -window_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title, smoothing = True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): List of results arrays to plot\n",
    "        legend_list (list): List of legends corresponding to each result array\n",
    "        color_list (list): List of color corresponding to each result array\n",
    "        ylabel (string): Label of the vertical axis\n",
    "\n",
    "        Make sure the elements in the arr_list, legend_list, and color_list\n",
    "        are associated with each other correctly (in the same order).\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # Set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the vertical labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # Plot results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # Compute the standard error (of raw data, not smoothed)\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # Plot the mean\n",
    "        averages = moving_average(arr.mean(axis=0)) if smoothing else arr.mean(axis=0)\n",
    "        h, = ax.plot(range(arr.shape[1]), averages, color=color, label=legend)\n",
    "        # Plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), averages - arr_err, averages + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # Save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # Plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy and value networks\n",
    "class REINFORCEBaselineNet(nn.Module):\n",
    "    def __init__(self, observation_dim=3, hidden_dim=128, action_dim=4):\n",
    "        super(REINFORCEBaselineNet, self).__init__()\n",
    "\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the critic network here. The architecture should be:\n",
    "\n",
    "                Layer 1: Linear, input size 3, output size 128\n",
    "                Activation 1: ReLU\n",
    "                Layer 2: Linear, input size 128, output size 1\n",
    "                Activation 2: Identity (or none)\n",
    "        \"\"\"\n",
    "        self.critic_in = nn.Linear(observation_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.critic_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the actor network here. The architecture should be (same as before):\n",
    "\n",
    "                Layer 1: Linear, input size 3, output size 128\n",
    "                Activation 1: ReLU\n",
    "                Layer 2: Linear, input size 128, output size 4\n",
    "                Activation 2: Softmax\n",
    "        \"\"\"\n",
    "        self.actor_in = nn.Linear(observation_dim, hidden_dim)\n",
    "        # reuse self.relu\n",
    "        self.actor_out = nn.Linear(hidden_dim, action_dim)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the forward propagation for both actor and critic networks\n",
    "        \"\"\"\n",
    "\n",
    "        critic_x = self.critic_in(x)\n",
    "        critic_x = self.relu(critic_x)\n",
    "        state_value = self.critic_out(critic_x)\n",
    "\n",
    "        actor_x = self.actor_in(x)\n",
    "        actor_x = self.relu(actor_x)\n",
    "        actor_x = self.actor_out(actor_x)\n",
    "        action_probs = self.softmax(actor_x)\n",
    "\n",
    "        return state_value, action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedActorCriticNet(nn.Module):\n",
    "    def __init__(self, observation_dim=3, hidden_dim=128, action_dim=4):\n",
    "        super(SharedActorCriticNet, self).__init__()\n",
    "\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the critic network here. The architecture should be:\n",
    "\n",
    "                Layer 1: Linear, input size 3, output size 128\n",
    "                Activation 1: ReLU\n",
    "                Layer 2: Linear, input size 128, output size 1\n",
    "                Activation 2: Identity (or none)\n",
    "        \"\"\"\n",
    "        self.shared_in = nn.Linear(observation_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.critic_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the actor network here. The architecture should be (same as before):\n",
    "\n",
    "                Layer 1: Linear, input size 3, output size 128\n",
    "                Activation 1: ReLU\n",
    "                Layer 2: Linear, input size 128, output size 4\n",
    "                Activation 2: Softmax\n",
    "        \"\"\"\n",
    "        # reuse shared_in, self.relu\n",
    "        self.actor_out = nn.Linear(hidden_dim, action_dim)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the forward propagation for both actor and critic networks\n",
    "        \"\"\"\n",
    "        # shared feature extraction layer\n",
    "        shared_x = self.shared_in(x)\n",
    "        shared_x = self.relu(shared_x)\n",
    "\n",
    "        # Compute state value estimates (critic network)\n",
    "        state_value = self.critic_out(shared_x)\n",
    "\n",
    "        # Compute action probabilities (actor network)\n",
    "        actor_x = self.actor_out(shared_x)\n",
    "        action_probs = self.softmax(actor_x)\n",
    "\n",
    "        return state_value, action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = namedtuple(\n",
    "    'Batch', ('states', 'actions', 'rewards', 'next_states', 'dones', 'log_probs', 'state_values')\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, max_size, state_size):\n",
    "        \"\"\"Replay memory implemented as a circular buffer.\n",
    "\n",
    "        Experiences will be removed in a FIFO manner after reaching maximum\n",
    "        buffer size.\n",
    "\n",
    "        Args:\n",
    "            - max_size: Maximum size of the buffer\n",
    "            - state_size: Size of the state-space features for the environment\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        # Preallocating all the required memory, for speed concerns\n",
    "        self.states = torch.empty((max_size, state_size))\n",
    "        self.actions = torch.empty((max_size, 1), dtype=torch.long)\n",
    "        self.rewards = torch.empty((max_size, 1))\n",
    "        self.next_states = torch.empty((max_size, state_size))\n",
    "        self.dones = torch.empty((max_size, 1), dtype=torch.bool)\n",
    "        # self.boostrap_returns = torch.empty((max_size, 1))\n",
    "        self.log_probs = torch.empty((max_size, 1))\n",
    "        self.state_values = torch.empty((max_size, 1))\n",
    "\n",
    "        # Pointer to the current location in the circular buffer\n",
    "        self.idx = 0\n",
    "        # Indicates number of transitions currently stored in the buffer\n",
    "        self.size = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done, log_prob, state_value):\n",
    "        \"\"\"Add a transition to the buffer.\n",
    "\n",
    "        :param state: 1-D np.ndarray of state-features\n",
    "        :param action: Integer action\n",
    "        :param reward: Float reward\n",
    "        :param next_state: 1-D np.ndarray of state-features\n",
    "        :param done: Boolean value indicating the end of an episode\n",
    "        :param bootstrap_return: reward + critic(s')\n",
    "        :param log_prob log of probability for selected action\n",
    "        :param state_value s\n",
    "        \"\"\"\n",
    "\n",
    "        if self.size == self.max_size:\n",
    "            return\n",
    "\n",
    "        # YOUR CODE HERE: Store the input values into the appropriate\n",
    "        # attributes, using the current buffer position `self.idx`\n",
    "\n",
    "        self.states[self.idx] = torch.tensor(state)\n",
    "        self.actions[self.idx] = action\n",
    "        self.rewards[self.idx] = reward\n",
    "        self.next_states[self.idx] = torch.tensor(next_state)\n",
    "        self.dones[self.idx] = done\n",
    "\n",
    "        # self.boostrap_returns[self.idx] = bootstrap_return\n",
    "        self.log_probs[self.idx] = log_prob\n",
    "        self.state_values[self.idx] = state_value\n",
    "\n",
    "        # DO NOT EDIT\n",
    "        # Circulate the pointer to the next position\n",
    "        self.idx = (self.idx + 1) % self.max_size\n",
    "        # Update the current buffer size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def get_all(self):\n",
    "        return Batch(\n",
    "            self.states[:self.size],\n",
    "            self.actions[:self.size],\n",
    "            self.rewards[:self.size],\n",
    "            self.next_states[:self.size],\n",
    "            self.dones[:self.size],\n",
    "            self.log_probs[:self.size],\n",
    "            self.state_values[:self.size]\n",
    "        )\n",
    "\n",
    "\n",
    "    def sample(self, batch_size) -> Batch:\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "\n",
    "        If the buffer contains less that `batch_size` transitions, sample all\n",
    "        of them.\n",
    "\n",
    "        :param batch_size: Number of transitions to sample\n",
    "        :rtype: Batch\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE: Randomly sample an appropriate number of\n",
    "        # transitions *without replacement*. If the buffer contains less than\n",
    "        # `batch_size` transitions, return all of them. The return type must\n",
    "        # be a `Batch`.\n",
    "\n",
    "        sample_size = min(self.size, batch_size)\n",
    "\n",
    "        sample_indices = torch.randint(0, self.size, size=(sample_size,), dtype=torch.long)\n",
    "        batch = Batch(\n",
    "            self.states[sample_indices],\n",
    "            self.actions[sample_indices],\n",
    "            self.rewards[sample_indices],\n",
    "            self.next_states[sample_indices],\n",
    "            self.dones[sample_indices],\n",
    "            self.log_probs[sample_indices],\n",
    "            self.state_values[sample_indices]\n",
    "        )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def populate(self, env, num_steps):\n",
    "        \"\"\"Populate this replay memory with `num_steps` from the random policy.\n",
    "\n",
    "        :param env: Gymnasium environment\n",
    "        :param num_steps: Number of steps to populate the replay memory\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE: Run a random policy for `num_steps` time-steps and\n",
    "        # populate the replay memory with the resulting transitions.\n",
    "        # Hint: Use the self.add() method.\n",
    "\n",
    "        # Random policy\n",
    "        def policy(state):\n",
    "            return env.action_space[torch.randint(len(env.action_space))]\n",
    "\n",
    "        # Basic gym loop\n",
    "        state, info = env.reset()\n",
    "        for i in range(num_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            self.add(state, action, reward, next_state, terminated)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                state, info = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "    def clear(self):\n",
    "        self.idx = 0\n",
    "        self.size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO agent - use joint or separate actor critic networks\n",
    "class PPOAgent(object):\n",
    "    def __init__(self, shared_backbone=False):\n",
    "        # Create the actor and critic networks\n",
    "        if shared_backbone:\n",
    "            self.policy_net = SharedActorCriticNet()\n",
    "            # cached target net\n",
    "            self.target_net = SharedActorCriticNet()\n",
    "        else:\n",
    "            self.policy_net = REINFORCEBaselineNet() # basically just an actor critic net\n",
    "            self.target_net = REINFORCEBaselineNet()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Sample an action from the actor network, return the action and its log probability,\n",
    "        # and return the state value according to the critic network\n",
    "        state_tensor = torch.tensor(state).float().view(1, -1)\n",
    "        state_value, action_probs = self.policy_net(state_tensor)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        return action.item(), log_prob, state_value\n",
    "\n",
    "    def get_estimates(self, state):\n",
    "        \"\"\"\n",
    "        Returns state value estimate and entropy of action distribution at a given state or batch of states\n",
    "        \"\"\"\n",
    "        # state_tensor = torch.tensor(state).float().view(1, -1)\n",
    "        state_value, action_probs = self.policy_net(state)\n",
    "        m = Categorical(action_probs)\n",
    "        return state_value, m.entropy()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        # cache the target network before training iterations to constrain probability ratio\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    @torch.no_grad\n",
    "    def get_action_probability(self, state, action):\n",
    "        # state_tensor = torch.tensor(state).float().view(1, -1)\n",
    "\n",
    "        # _, old_action_probs = self.target_net(state_tensor)\n",
    "        _, current_action_probs = self.policy_net(state)\n",
    "        return current_action_probs[..., action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Trainer\n",
    "class PPOAgentTrainer(object):\n",
    "    def __init__(self, agent: PPOAgent, env: FourRooms | gym.Env, params):\n",
    "        # Agent object\n",
    "        self.agent = agent\n",
    "\n",
    "        # Environment object\n",
    "        self.env = env\n",
    "\n",
    "        # Training parameters\n",
    "        self.params = params\n",
    "\n",
    "        # Lists to store the log probabilities, state values, and rewards for one episode\n",
    "        self.saved_log_probs = []\n",
    "        self.saved_state_values = []\n",
    "        self.saved_rewards = []\n",
    "        self.saved_bootstrap_returns = []\n",
    "        self.replay_buffer = ReplayMemory(max_size=params['buffer_size'], state_size=3)\n",
    "\n",
    "        # Gamma\n",
    "        self.gamma = params['gamma']\n",
    "\n",
    "        # epsilon for PPO-Clip\n",
    "        self.epsilon = params['epsilon']\n",
    "\n",
    "        self.k_epochs = params['epochs_per_update']\n",
    "\n",
    "        self.entropy_coef = params['entropy_coef']\n",
    "\n",
    "\n",
    "        # Create the optimizer\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the Adam optimizer with the learning rate in params\n",
    "        \"\"\"\n",
    "        self.optimizer = torch.optim.Adam(agent.policy_net.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_state_feature(state):\n",
    "        return [state[0] / 10, state[1] / 10, 1]\n",
    "\n",
    "    def update_actor_critic_networks(self):\n",
    "\n",
    "        # List to store the policy loss for each time step\n",
    "        # policy_loss = []\n",
    "\n",
    "        # List to store the value loss for each time step\n",
    "        # value_loss = []\n",
    "\n",
    "        # entropy_bonus = 0\n",
    "\n",
    "        dataset = self.replay_buffer.get_all()\n",
    "\n",
    "        # Compute discounted returns at each time step (rewards-to-go)\n",
    "        G = 0\n",
    "        returns = deque()\n",
    "\n",
    "        for reward, done in zip(reversed(dataset.rewards), reversed(dataset.dones)):\n",
    "            if done:\n",
    "                # end of episode, reset discounted return calculation\n",
    "                G = 0\n",
    "\n",
    "            G = self.gamma * G + reward\n",
    "            returns.appendleft(G)\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        print(\"Updating networks, returns: \", returns)\n",
    "\n",
    "        for epoch in range(self.k_epochs):\n",
    "            # for state, action, log_prob, val, r in zip(dataset.states, dataset.actions, dataset.log_probs, dataset.state_values, returns):\n",
    "            # Compute the policy and value loss for each time step\n",
    "            # (joint optimization)\n",
    "            advantage = (returns - dataset.state_values.detach()) # detach state value, don't optimize value network with policy loss\n",
    "\n",
    "            # Probability ratio of selecting action at s, use log_prob for pi_theta_old\n",
    "            ratio_t = self.agent.get_action_probability(dataset.states, dataset.actions) / torch.exp(dataset.log_probs.detach())\n",
    "\n",
    "            print(f\"{ratio_t.shape=}\")\n",
    "\n",
    "\n",
    "            # Unclipped surrogate advantage\n",
    "            unclipped_adv = ratio_t * advantage\n",
    "\n",
    "            # Clipped surrogate advantage\n",
    "            clipped_adv = torch.clamp(ratio_t, min=(1 - self.epsilon), max=(1 + self.epsilon))\n",
    "\n",
    "            # Choose the minimum of the two (in the negative direction, if we choose a bad action, it should be bad)\n",
    "            # unclipped chosen when ratio > 1+epsilon and advantage is negative\n",
    "            # or when ratio < 1 - epsilon and advantage is positive\n",
    "            # negative sign because torch Adam minimizes\n",
    "            policy_loss = -torch.min(unclipped_adv, clipped_adv)\n",
    "\n",
    "            latest_state_values, entropy_bonus = self.agent.get_estimates(dataset.states)\n",
    "\n",
    "            assert not torch.isnan(dataset.log_probs).any(), \"Found NaN in log_probs\"\n",
    "            assert not torch.isnan(ratio_t).any(), \"Found NaN in ratio_t\"\n",
    "            assert not torch.isnan(entropy_bonus).any(), \"Found NaN in entropy_bonus\"\n",
    "\n",
    "            # Critic / value function estimate loss based on monte carlo return\n",
    "            value_loss = F.mse_loss(latest_state_values, returns.detach().unsqueeze(-1))\n",
    "\n",
    "            # Compute the total loss\n",
    "            total_loss = policy_loss.sum() + value_loss.sum() - (self.entropy_coef * entropy_bonus.sum())\n",
    "            # total_loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum() - entropy_bonus\n",
    "\n",
    "            pdb.set_trace()\n",
    "\n",
    "            \"\"\" YOUR CODE HERE:\n",
    "                    Implement one step of backpropagation (gradient descent)\n",
    "            \"\"\"\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            for name, param in self.agent.policy_net.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(name, param.grad.abs().max())\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # After backpropagation, clear the data\n",
    "        self.replay_buffer.clear()\n",
    "        # del self.saved_log_probs[:]\n",
    "        # del self.saved_state_values[:]\n",
    "        # del self.saved_rewards[:]\n",
    "        # del self.saved_bootstrap_returns[:]\n",
    "\n",
    "        return G, total_loss.item()\n",
    "\n",
    "    def run_train(self):\n",
    "        # Lists to store the returns and losses during the training\n",
    "        train_returns = []\n",
    "        train_losses = []\n",
    "\n",
    "        self.replay_buffer.clear()\n",
    "\n",
    "        # Training loop\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        G, loss = 0, 0\n",
    "\n",
    "        t_bar = tqdm.trange(self.params['train_timesteps'])\n",
    "        for t in t_bar:\n",
    "            \"\"\" YOUR CODE HERE:\n",
    "                    Implement the Actor-Critic algorithm here.\n",
    "            \"\"\"\n",
    "            # Collect one transition\n",
    "            action, log_prob, state_value = self.agent.get_action(self.compute_state_feature(state))\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            state_tensor = self.compute_state_feature(state)\n",
    "\n",
    "            self.replay_buffer.add(state_tensor, action, reward, self.compute_state_feature(next_state), done, log_prob, state_value)\n",
    "\n",
    "            if not done:\n",
    "                state = next_state\n",
    "            else:\n",
    "                # reset environment\n",
    "                state, info = self.env.reset()\n",
    "                done = False\n",
    "\n",
    "            if t % self.params['update_frequency'] == 0: # PPO update every N iterations\n",
    "                # update both network weights\n",
    "                G, loss = self.update_actor_critic_networks()\n",
    "\n",
    "                # Save the return and loss\n",
    "                train_returns.append(G)\n",
    "                train_losses.append(loss)\n",
    "\n",
    "            # Add description\n",
    "            t_bar.set_description(f\"Timestep: {t} | Return: {G} | Loss: {loss:.2f}\")\n",
    "\n",
    "        return train_returns, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    my_env = FourRooms()\n",
    "\n",
    "    train_params = {\n",
    "        'num_episodes': 10000,\n",
    "        'num_trials': 1,\n",
    "        'learning_rate': 1e-3,\n",
    "        'gamma': 0.99,\n",
    "        'train_timesteps': 10000,\n",
    "        'entropy_coef': 0.01,\n",
    "        'buffer_size': my_env.max_time_steps * 4,\n",
    "        'update_frequency': my_env.max_time_steps * 4,\n",
    "        'epsilon': 0.2,\n",
    "        'epochs_per_update': 5\n",
    "    }\n",
    "\n",
    "    ppo_returns = []\n",
    "    ppo_losses = []\n",
    "    for _ in range(train_params['num_trials']):\n",
    "        my_agent = PPOAgent() # agent is the same\n",
    "        my_trainer = PPOAgentTrainer(my_agent, my_env, train_params)\n",
    "        returns, losses = my_trainer.run_train()\n",
    "\n",
    "        ppo_returns.append(returns)\n",
    "        ppo_losses.append(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
